{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ad372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47bde88-f05a-4121-941a-d7da5da6d4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:04:54.335215Z",
     "iopub.status.busy": "2024-01-24T09:04:54.334846Z",
     "iopub.status.idle": "2024-01-24T09:05:02.573441Z",
     "shell.execute_reply": "2024-01-24T09:05:02.572662Z",
     "shell.execute_reply.started": "2024-01-24T09:04:54.335150Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /opt/saturncloud/envs/saturn/lib/python3.10/site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5dc2a3-e718-4329-a17f-d6ecf47ee266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:02.575458Z",
     "iopub.status.busy": "2024-01-24T09:05:02.574870Z",
     "iopub.status.idle": "2024-01-24T09:05:02.867965Z",
     "shell.execute_reply": "2024-01-24T09:05:02.865698Z",
     "shell.execute_reply.started": "2024-01-24T09:05:02.575423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bd6796-c5ee-4fd4-88c3-7e4e654918ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:02.870123Z",
     "iopub.status.busy": "2024-01-24T09:05:02.869372Z",
     "iopub.status.idle": "2024-01-24T09:05:02.882311Z",
     "shell.execute_reply": "2024-01-24T09:05:02.880481Z",
     "shell.execute_reply.started": "2024-01-24T09:05:02.870091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('../Final_Intents.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48803f0-81ec-48c2-a4c6-f5079c86b071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:02.894740Z",
     "iopub.status.busy": "2024-01-24T09:05:02.894028Z",
     "iopub.status.idle": "2024-01-24T09:05:02.924400Z",
     "shell.execute_reply": "2024-01-24T09:05:02.923047Z",
     "shell.execute_reply.started": "2024-01-24T09:05:02.894697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to pair each question with its corresponding response\n",
    "def pair_questions_responses(data):\n",
    "    paired_data = []\n",
    "    for item in data:\n",
    "        tag = item.get('tag', 'Unknown')\n",
    "        questions = item.get('questions', [])\n",
    "        responses = item.get('responses', [])\n",
    "\n",
    "        for question, response in zip(questions, responses):\n",
    "            paired_data.append({'tag': tag, 'question': question, 'response': response})\n",
    "\n",
    "    return paired_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef5811e-0561-401b-8ba2-f623a189bff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:02.928620Z",
     "iopub.status.busy": "2024-01-24T09:05:02.928179Z",
     "iopub.status.idle": "2024-01-24T09:05:02.977146Z",
     "shell.execute_reply": "2024-01-24T09:05:02.975408Z",
     "shell.execute_reply.started": "2024-01-24T09:05:02.928583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>What does the data science course at Moringa S...</td>\n",
       "      <td>The data science course at Moringa School cove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Can you provide details about the curriculum a...</td>\n",
       "      <td>The curriculum and modules in the data science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>How long is the data science course, and what ...</td>\n",
       "      <td>The duration of the data science course is fle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Tell me about the practical aspects of the dat...</td>\n",
       "      <td>Practical aspects of the data science learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Are there any prerequisites for enrolling in t...</td>\n",
       "      <td>Prerequisites for enrolling in the data scienc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tag                                           question  \\\n",
       "0  Data Science  What does the data science course at Moringa S...   \n",
       "1  Data Science  Can you provide details about the curriculum a...   \n",
       "2  Data Science  How long is the data science course, and what ...   \n",
       "3  Data Science  Tell me about the practical aspects of the dat...   \n",
       "4  Data Science  Are there any prerequisites for enrolling in t...   \n",
       "\n",
       "                                            response  \n",
       "0  The data science course at Moringa School cove...  \n",
       "1  The curriculum and modules in the data science...  \n",
       "2  The duration of the data science course is fle...  \n",
       "3  Practical aspects of the data science learning...  \n",
       "4  Prerequisites for enrolling in the data scienc...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_data = pair_questions_responses(data)\n",
    "df = pd.DataFrame(paired_data)\n",
    "df.head ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33dba2fd-7a4b-4425-bd61-4aab26d83112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:02.987117Z",
     "iopub.status.busy": "2024-01-24T09:05:02.986653Z",
     "iopub.status.idle": "2024-01-24T09:05:06.080229Z",
     "shell.execute_reply": "2024-01-24T09:05:06.078738Z",
     "shell.execute_reply.started": "2024-01-24T09:05:02.987084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to questions and responses\n",
    "df['question'] = df['question'].apply(preprocess_text)\n",
    "df['response'] = df['response'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f22cb9-3f72-4ac6-b8f0-b7f0c00ed485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.082311Z",
     "iopub.status.busy": "2024-01-24T09:05:06.081978Z",
     "iopub.status.idle": "2024-01-24T09:05:06.096557Z",
     "shell.execute_reply": "2024-01-24T09:05:06.095048Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.082280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a764777-2617-4c71-a9ee-5a6bccfac746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.098420Z",
     "iopub.status.busy": "2024-01-24T09:05:06.097736Z",
     "iopub.status.idle": "2024-01-24T09:05:06.135546Z",
     "shell.execute_reply": "2024-01-24T09:05:06.133342Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.098385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['question'].tolist() + train_data['response'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93ab925-f683-41ef-a1cc-8d95a8ab7f6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.137645Z",
     "iopub.status.busy": "2024-01-24T09:05:06.137077Z",
     "iopub.status.idle": "2024-01-24T09:05:06.168498Z",
     "shell.execute_reply": "2024-01-24T09:05:06.166245Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.137611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert texts to sequences\n",
    "train_questions_seq = tokenizer.texts_to_sequences(train_data['question'].tolist())\n",
    "train_responses_seq = tokenizer.texts_to_sequences(train_data['response'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6357b116-3740-4fc0-b8f1-0a86df232920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.180829Z",
     "iopub.status.busy": "2024-01-24T09:05:06.180156Z",
     "iopub.status.idle": "2024-01-24T09:05:06.186547Z",
     "shell.execute_reply": "2024-01-24T09:05:06.185236Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.180786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the maximum sequence length\n",
    "max_seq_length = max(max(len(seq) for seq in train_questions_seq), max(len(seq) for seq in train_responses_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9d8bcf1-3f7d-4a98-b939-39947be109a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.190410Z",
     "iopub.status.busy": "2024-01-24T09:05:06.189921Z",
     "iopub.status.idle": "2024-01-24T09:05:06.206923Z",
     "shell.execute_reply": "2024-01-24T09:05:06.205725Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.190381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "train_questions_padded = pad_sequences(train_questions_seq, maxlen=max_seq_length, padding='post')\n",
    "train_responses_padded = pad_sequences(train_responses_seq, maxlen=max_seq_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f201e0d-e195-4bd6-a797-70ddc97069da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.218922Z",
     "iopub.status.busy": "2024-01-24T09:05:06.218540Z",
     "iopub.status.idle": "2024-01-24T09:05:06.249769Z",
     "shell.execute_reply": "2024-01-24T09:05:06.248304Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.218884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "test_questions_seq = tokenizer.texts_to_sequences(test_data['question'].tolist())\n",
    "test_responses_seq = tokenizer.texts_to_sequences(test_data['response'].tolist())\n",
    "test_questions_padded = pad_sequences(test_questions_seq, maxlen=max_seq_length, padding='post')\n",
    "test_responses_padded = pad_sequences(test_responses_seq, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e62942eb-8abf-4872-886d-f8aecfc2cf1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.252210Z",
     "iopub.status.busy": "2024-01-24T09:05:06.251668Z",
     "iopub.status.idle": "2024-01-24T09:05:06.356246Z",
     "shell.execute_reply": "2024-01-24T09:05:06.355324Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.252176Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode the responses\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "train_responses_one_hot = np.zeros((len(train_responses_padded), max_seq_length, vocab_size))\n",
    "\n",
    "for i, sequence in enumerate(train_responses_padded):\n",
    "    for j, word_index in enumerate(sequence):\n",
    "        train_responses_one_hot[i, j, word_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6925daec-d612-4057-93f9-9dc71eed9d8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.357942Z",
     "iopub.status.busy": "2024-01-24T09:05:06.357589Z",
     "iopub.status.idle": "2024-01-24T09:05:06.368023Z",
     "shell.execute_reply": "2024-01-24T09:05:06.367188Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.357874Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81fe294d-e0f0-4feb-bf59-cdb763c83ffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:06.369985Z",
     "iopub.status.busy": "2024-01-24T09:05:06.369502Z",
     "iopub.status.idle": "2024-01-24T09:05:07.408968Z",
     "shell.execute_reply": "2024-01-24T09:05:07.406721Z",
     "shell.execute_reply.started": "2024-01-24T09:05:06.369955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 09:05:06.394402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Updated Model Architecture\n",
    "embedding_dim = 128  # Embedding dimensionality\n",
    "latent_dim = 256 \n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_seq_length,))\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Bahdanau Attention\n",
    "attention_layer = BahdanauAttention(latent_dim)\n",
    "context_vector, attention_weights = attention_layer(state_h, encoder_outputs)\n",
    "\n",
    "# Decoder with attention\n",
    "decoder_inputs = Input(shape=(max_seq_length,))\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "context_vector_with_time_axis = tf.expand_dims(context_vector, 1)\n",
    "\n",
    "# Broadcast the context vector to have the same shape as the decoder embedding\n",
    "sequence_length = tf.shape(decoder_embedding)[1]\n",
    "context_vector_broadcasted = tf.broadcast_to(context_vector_with_time_axis, [tf.shape(context_vector_with_time_axis)[0], sequence_length, tf.shape(context_vector_with_time_axis)[-1]])\n",
    "decoder_input_combined = tf.concat([context_vector_broadcasted, decoder_embedding], axis=-1)\n",
    "\n",
    "# Continue with the decoder LSTM, etc.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_input_combined, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a0e1c8-c81c-4a90-b6cf-c7514a14b472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:07.410756Z",
     "iopub.status.busy": "2024-01-24T09:05:07.410281Z",
     "iopub.status.idle": "2024-01-24T09:05:07.428969Z",
     "shell.execute_reply": "2024-01-24T09:05:07.428163Z",
     "shell.execute_reply.started": "2024-01-24T09:05:07.410723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model with Adam optimizer and use categorical_crossentropy as loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2389e1f1-e8b7-4a13-8b61-bd55f7e2c636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:07.430704Z",
     "iopub.status.busy": "2024-01-24T09:05:07.430228Z",
     "iopub.status.idle": "2024-01-24T09:05:07.473384Z",
     "shell.execute_reply": "2024-01-24T09:05:07.471685Z",
     "shell.execute_reply.started": "2024-01-24T09:05:07.430673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 33)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 33, 128)      169856      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        394240      ['embedding[0][0]']              \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " bahdanau_attention (BahdanauAt  ((None, 256),       131841      ['lstm[0][1]',                   \n",
      " tention)                        (None, None, 1))                 'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 33)]         0           []                               \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 256)       0           ['bahdanau_attention[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 33, 128)      169856      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOpLamb  (3,)                0           ['tf.expand_dims[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (3,)                0           ['embedding_1[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_2 (TFOpLamb  (3,)                0           ['tf.expand_dims[0][0]']         \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  ()                  0           ['tf.compat.v1.shape_1[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  ()                  0           ['tf.compat.v1.shape_2[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.broadcast_to (TFOpLambda)   (None, 33, 256)      0           ['tf.expand_dims[0][0]',         \n",
      "                                                                  'tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.__operators__.getitem_2[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 33, 384)      0           ['tf.broadcast_to[0][0]',        \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 33, 256),    656384      ['tf.concat[0][0]',              \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 33, 1327)     341039      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,863,216\n",
      "Trainable params: 1,863,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c9665-01f8-43b8-8808-f553ed85c067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-24T09:05:07.474796Z",
     "iopub.status.busy": "2024-01-24T09:05:07.474509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 [==============================] - 15s 1s/step - loss: 6.7906 - accuracy: 0.4627 - val_loss: 4.4882 - val_accuracy: 0.5469\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 7s 1s/step - loss: 3.6837 - accuracy: 0.5774 - val_loss: 3.4101 - val_accuracy: 0.5506\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 7s 1s/step - loss: 3.0613 - accuracy: 0.5805 - val_loss: 3.2857 - val_accuracy: 0.5517\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 7s 1s/step - loss: 2.9680 - accuracy: 0.5817 - val_loss: 3.2684 - val_accuracy: 0.5517\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 4s 575ms/step - loss: 2.9352 - accuracy: 0.5819 - val_loss: 3.2887 - val_accuracy: 0.5517\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 3s 547ms/step - loss: 2.9063 - accuracy: 0.5832 - val_loss: 3.2952 - val_accuracy: 0.5531\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 3s 573ms/step - loss: 2.8848 - accuracy: 0.5842 - val_loss: 3.3029 - val_accuracy: 0.5528\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 3s 571ms/step - loss: 2.8844 - accuracy: 0.5845 - val_loss: 3.3156 - val_accuracy: 0.5531\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 3s 557ms/step - loss: 2.8682 - accuracy: 0.5841 - val_loss: 3.3115 - val_accuracy: 0.5539\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 3s 586ms/step - loss: 2.8378 - accuracy: 0.5848 - val_loss: 3.3266 - val_accuracy: 0.5531\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 3s 564ms/step - loss: 2.8347 - accuracy: 0.5840 - val_loss: 3.3367 - val_accuracy: 0.5531\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 3s 545ms/step - loss: 2.8193 - accuracy: 0.5848 - val_loss: 3.3405 - val_accuracy: 0.5528\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 3s 572ms/step - loss: 2.8043 - accuracy: 0.5850 - val_loss: 3.3406 - val_accuracy: 0.5549\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 3s 566ms/step - loss: 2.7957 - accuracy: 0.5840 - val_loss: 3.3533 - val_accuracy: 0.5539\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 3s 551ms/step - loss: 2.7811 - accuracy: 0.5846 - val_loss: 3.3556 - val_accuracy: 0.5535\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 3s 562ms/step - loss: 2.7718 - accuracy: 0.5848 - val_loss: 3.3557 - val_accuracy: 0.5546\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 3s 574ms/step - loss: 2.7574 - accuracy: 0.5852 - val_loss: 3.3476 - val_accuracy: 0.5553\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 3s 543ms/step - loss: 2.7421 - accuracy: 0.5852 - val_loss: 3.3617 - val_accuracy: 0.5553\n",
      "Epoch 19/20\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit([train_questions_padded, train_responses_padded], train_responses_one_hot,\n",
    "                    batch_size=64,\n",
    "                    epochs=20,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9863a7-43f2-4755-bd5e-d2752906532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_responses_one_hot = np.zeros((len(test_responses_padded), max_seq_length, vocab_size))\n",
    "\n",
    "for i, sequence in enumerate(test_responses_padded):\n",
    "    for j, word_index in enumerate(sequence):\n",
    "        test_responses_one_hot[i, j, word_index] = 1\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate([test_questions_padded, test_responses_padded], test_responses_one_hot)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23d595-773c-44e5-9530-df92a0a35113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = model.evaluate([train_questions_padded, train_responses_padded], train_responses_one_hot)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
