{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8799c1c",
   "metadata": {},
   "source": [
    "### Expolatory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6caa8",
   "metadata": {},
   "source": [
    "* Project Overview\n",
    "\n",
    "The project aims to create a self-learning AI Chat-Bot for Moringa School using the Natural Language Toolkit (NLTK). This Chat-Bot is designed to enhance user interaction on the Moringa School website by providing information, answering inquiries, and improving engagement through natural language processing.\n",
    "* Objectives of the EDA\n",
    "\n",
    "The primary objective of the EDA is to thoroughly examine the dataset comprising intents, questions, and responses. The analysis aims to understand the distribution and nature of the data, identify patterns, and uncover insights that will guide the development of the AI Chat-Bot. Key areas include understanding the diversity of intents, analyzing the structure and length of questions and responses, and identifying common themes and word frequencies.\n",
    "* Data Visualization\n",
    "\n",
    "Data visualization plays a crucial role in this EDA. Various graphical representations will be employed to illustrate the findings. These include bar charts to show intent distribution, histograms for analyzing question and response lengths, and word clouds for visualizing frequent terms. These visual tools will help in making data-driven decisions and in conveying complex data in an accessible format for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# import libraries\n",
    "import json\n",
    "import re\n",
    "import nltk;\n",
    "from nltk.stem import PorterStemmer;\n",
    "nltk.download('vader_lexicon');\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer;\n",
    "from nltk.tokenize import word_tokenize;\n",
    "from nltk.tokenize import RegexpTokenizer;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.stem import WordNetLemmatizer;\n",
    "nltk.download('wordnet');\n",
    "from nltk import bigrams, trigrams, FreqDist;\n",
    "nltk.download('stopwords');\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "nltk.download('punkt');\n",
    "from collections import Counter;\n",
    "import numpy as np\n",
    "!pip install wordcloud;\n",
    "from scipy import stats\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "with open('../Final_Intents.json', 'r') as file:\n",
    "    intents = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process and analyze the data for display\n",
    "def process_intents(intents):\n",
    "    for intent in intents:\n",
    "        print(f\"Intent: {intent.get('tag', 'No tag available')}\")\n",
    "        if 'questions' in intent:\n",
    "            print(\"Questions:\")\n",
    "            for question in intent['questions']:\n",
    "                print(f\" - {question}\")\n",
    "        else:\n",
    "            print(\"Questions: No questions available\")\n",
    "\n",
    "        if 'responses' in intent:\n",
    "            print(\"Responses:\")\n",
    "            for response in intent['responses']:\n",
    "                print(f\" - {response}\")\n",
    "        else:\n",
    "            print(\"Responses: No responses available\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Call the function with your intents list\n",
    "process_intents(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f037664",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization: Convert text to lowercase\n",
    "normalized_questions = [question.lower() for intent in intents for question in intent['questions']]\n",
    "# Display the first few normalized questions\n",
    "print(\"Normalized Questions (Sample):\", normalized_questions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615240bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation and special characters\n",
    "cleaned_questions = [re.sub(r'[^\\w\\s]', '', question) for question in normalized_questions]\n",
    "print(\"Cleaned Questions (Sample):\", cleaned_questions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1957ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: Break text into words\n",
    "tokenized_questions = [question.split() for question in cleaned_questions]\n",
    "# Display the first few tokenized questions\n",
    "print(\"Tokenized Questions (Sample):\", tokenized_questions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23798d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [[word for word in question if word not in stop_words] for question in tokenized_questions]\n",
    "# Display the first few sets of filtered words\n",
    "print(\"Filtered Words (Sample):\", filtered_words[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93169c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [[lemmatizer.lemmatize(word) for word in question] for question in filtered_words]\n",
    "\n",
    "print(\"Lemmatized Words (Sample):\", lemmatized_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [[stemmer.stem(word) for word in question] for question in filtered_words]\n",
    "# Display the first few sets of stemmed words\n",
    "print(\"Stemmed Words (Sample):\", stemmed_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9170849",
   "metadata": {},
   "source": [
    "## EDA\n",
    "#### Intent distribution\n",
    "We analyze the frequency of different intents. This can help use understand which topics are more common and might require more training data or refined responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104610a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for the intents\n",
    "intent_counts = {intent['tag']: len(intent['questions']) for intent in intents}\n",
    "\n",
    "# Calculate additional statistics\n",
    "mean_questions_per_intent = sum(intent_counts.values()) / len(intent_counts)\n",
    "median_questions_per_intent = sorted(intent_counts.values())[len(intent_counts) // 2]\n",
    "total_intents = len(intent_counts)\n",
    "\n",
    "# Find the intent(s) with the fewest questions\n",
    "min_questions_count = min(intent_counts.values())\n",
    "intents_with_fewest_questions = [intent for intent, count in intent_counts.items() if count == min_questions_count]\n",
    "\n",
    "# Print the calculated statistics\n",
    "print(f\"Total Intents: {total_intents}\")\n",
    "print(f\"Mean Questions per Intent: {mean_questions_per_intent:.2f}\")\n",
    "print(f\"Median Questions per Intent: {median_questions_per_intent}\")\n",
    "print(f\"Intent(s) with Fewest Questions ({min_questions_count} questions): {', '.join(intents_with_fewest_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed5a06",
   "metadata": {},
   "source": [
    "* Findings;\n",
    "* There are a total of 75 unique intents in the dataset. Each intent represents a specific category or topic.\n",
    "* On average, each intent contains approximately 8.69 questions. This statistic provides an understanding of the typical number of questions associated with each intent.\n",
    "* The median number of questions per intent is 10. This indicates that half of the intents have 10 or fewer questions, while the other half has more than 10 questions. The median offers insight into the central tendency of question counts.\n",
    "* There are several intents that have the fewest questions, each containing only 5 questions. These intents may represent topics or categories with relatively less content or focus compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2be92",
   "metadata": {},
   "source": [
    "##### Visualizing the First 10 and Last 10 Intents by Number of Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e177ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_intents = sorted(intent_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Extract top 10 and last 10 intents\n",
    "top_10_intents = sorted_intents[:10]\n",
    "last_10_intents = sorted_intents[-10:]\n",
    "combined_intents = top_10_intents + last_10_intents\n",
    "\n",
    "# Unpack the combined intents for plotting\n",
    "tags, counts = zip(*combined_intents)\n",
    "\n",
    "# Bar Chart for Top 10 and Last 10 Intent Counts - Rotated\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(tags, counts, color=['skyblue']*10 + ['orange']*10)\n",
    "plt.ylabel('Intents')\n",
    "plt.xlabel('Number of Questions')\n",
    "plt.title('Top 10 and Last 10 Intents by Number of Questions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b242326",
   "metadata": {},
   "source": [
    "###  Question and Response Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3326a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lengths of questions in all intents\n",
    "# Pass space character \n",
    "question_lengths = [len(question.split()) for intent in intents for question in intent['questions']]\n",
    "response_lengths = [len(response.split()) for intent in intents for response in intent.get('responses', [])]\n",
    "# Calculate the mean, median, and standard deviation for question lengths\n",
    "question_mean = statistics.mean(question_lengths)\n",
    "question_median = statistics.median(question_lengths)\n",
    "question_std_dev = statistics.stdev(question_lengths)\n",
    "\n",
    "# Calculate the mean, median, and standard deviation for response lengths\n",
    "response_mean = statistics.mean(response_lengths)\n",
    "response_median = statistics.median(response_lengths)\n",
    "response_std_dev = statistics.stdev(response_lengths)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Statistics for Question Lengths:\")\n",
    "print(f\"Mean: {question_mean}\")\n",
    "print(f\"Median: {question_median}\")\n",
    "print(f\"Standard Deviation: {question_std_dev}\")\n",
    "print(\"\\n\")\n",
    "print(\"Statistics for Response Lengths:\")\n",
    "print(f\"Mean: {response_mean}\")\n",
    "print(f\"Median: {response_median}\")\n",
    "print(f\"Standard Deviation: {response_std_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1d552",
   "metadata": {},
   "source": [
    "Questions: The statistics for question lengths show that, on average, questions are relatively short (around 10 words), with a moderate level of variability.\n",
    "Responses: In contrast, responses are longer on average (around 21 words), with greater variability in their lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the length of questions and responses\n",
    "question_lengths = [len(question.split()) for intent in intents for question in intent['questions']]\n",
    "response_lengths = [len(response.split()) for intent in intents for response in intent.get('responses', [])]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(question_lengths, bins=20, alpha=0.5, label='Questions')\n",
    "plt.hist(response_lengths, bins=20, alpha=0.5, label='Responses')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Question and Response Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38635b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(data=[question_lengths, response_lengths], inner=\"quartile\")\n",
    "plt.xticks([0, 1], ['Questions', 'Responses'])\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Distribution of Word Counts in Questions and Responses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52c61a",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis\n",
    "** Identify the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating word frequency\n",
    "all_words = [word for question in stemmed_words for word in question]  # Flatten the list\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display the most common words\n",
    "print(\"Most Common Words:\", word_freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c274ad",
   "metadata": {},
   "source": [
    "* findings;\n",
    "* The analysis provides insights into the key themes and topics present in the dataset.\n",
    "* The most common words reflect a strong emphasis on courses, data science, Moringa, and educational aspects.\n",
    "* With 'cours' being the most frequent word, the dataset places a substantial emphasis on educational offerings.\n",
    "* 'data,' 'scienc,' and 'moringa' indicate a strong focus on data science education, aligning with industry and institutional themes.\n",
    "* The repetition of terms like 'student,' 'develop,' and 'learn' underscores a learner-centric approach in the dataset.\n",
    "* Understanding these frequently occurring words is valuable for gaining insights into the main subjects and focus areas of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4ecf0",
   "metadata": {},
   "source": [
    "#### Visualizing Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top 20 most common words\n",
    "most_common_words = Counter(dict(word_freq)).most_common(20)\n",
    "# Unpacking words and frequencies\n",
    "words, frequencies = zip(*most_common_words)\n",
    "\n",
    "# Creating subplots with two columns\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 8))\n",
    "\n",
    "# Bar plot for the top 20 most common words\n",
    "axes[0].bar(words, frequencies, color='skyblue')\n",
    "axes[0].set_xlabel('Words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_xticklabels(words, rotation=90)\n",
    "axes[0].set_title('Top 20 Most Common Words in Questions')\n",
    "\n",
    "# Bubble chart for the top 20 most common words\n",
    "bubble_colors = [plt.cm.viridis(i) for i in range(len(words))]  # Use colormap for bubble colors\n",
    "axes[1].scatter(words, [1]*len(words), s=[freq*5 for freq in frequencies], alpha=0.7, color=bubble_colors)\n",
    "axes[1].set_xlabel('Words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Word Bubble Visualization')\n",
    "axes[1].set_xticklabels(words, rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a5125",
   "metadata": {},
   "source": [
    "### Word Cloud for Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08df136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word cloud for visualizing the most frequent words.\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a word cloud\n",
    "all_words_string = ' '.join([word for sublist in stemmed_words for word in sublist])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(all_words_string)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577d3fd",
   "metadata": {},
   "source": [
    "### Bi-grams and Tri-grams Analysis\n",
    "Analyze the most common bi-grams and tri-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe45d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create bi-grams and tri-grams\n",
    "bi_grams = list(bigrams(all_words))\n",
    "tri_grams = list(trigrams(all_words))\n",
    "\n",
    "# Frequency distribution\n",
    "bi_gram_freq = FreqDist(bi_grams)\n",
    "tri_gram_freq = FreqDist(tri_grams)\n",
    "\n",
    "# Display most common bi-grams and tri-grams\n",
    "print(\"Most Common Bi-grams:\", bi_gram_freq.most_common(10))\n",
    "print(\"Most Common Tri-grams:\", tri_gram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143c927",
   "metadata": {},
   "source": [
    "* Findings\n",
    "* The most common bi-gram is 'data science' with a frequency of 136, indicating a strong association between these two words.\n",
    "* 'Moringa school' is also highly frequent, suggesting a focus on education or programs related to Moringa School.\n",
    "* In tri-grams, 'data science course' is the most common, reinforcing the emphasis on data science education.\n",
    "* 'Moringa school' appears in both bi-grams and tri-grams, indicating its relevance and prominence in the dataset.\n",
    "* Other terms like 'mobile development,' 'software engineering,' and 'financial aid' also stand out in the most common n-grams.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed0278",
   "metadata": {},
   "source": [
    "#### Visualizing the N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_labels, bi_gram_counts = zip(*bi_gram_freq.most_common(10))\n",
    "\n",
    "# Extracting labels and frequencies for tri-grams\n",
    "tri_gram_labels, tri_gram_counts = zip(*tri_gram_freq.most_common(10))\n",
    "\n",
    "# Plotting the bar plot with two columns\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "# Bar plot for bi-grams\n",
    "ax1.bar(range(len(bi_gram_labels)), bi_gram_counts, color='skyblue')\n",
    "ax1.set_xlabel('Bi-grams')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_xticks(range(len(bi_gram_labels)))\n",
    "ax1.set_xticklabels(bi_gram_labels, rotation=45, ha='right')\n",
    "ax1.set_title('Most Common Bi-grams')\n",
    "\n",
    "# Bar plot for tri-grams\n",
    "ax2.bar(range(len(tri_gram_labels)), tri_gram_counts, color='salmon')\n",
    "ax2.set_xlabel('Tri-grams')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_xticks(range(len(tri_gram_labels)))\n",
    "ax2.set_xticklabels(tri_gram_labels, rotation=45, ha='right')\n",
    "ax2.set_title('Most Common Tri-grams')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
