{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe08e5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "967fcfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "with open('../Final_Intents.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350cc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pair each question with its corresponding response\n",
    "def pair_questions_responses(data):\n",
    "    paired_data = []\n",
    "    for item in data:\n",
    "        tag = item.get('tag', 'Unknown')\n",
    "        questions = item.get('questions', [])\n",
    "        responses = item.get('responses', [])\n",
    "        \n",
    "        for question, response in zip(questions, responses):\n",
    "            paired_data.append({'tag': tag, 'question': question, 'response': response})\n",
    "    \n",
    "    return paired_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ed57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_data = pair_questions_responses(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fb13ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>What is the fundamental starting point for und...</td>\n",
       "      <td>The fundamental starting point for understandi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>How does Moringa School introduce the concept ...</td>\n",
       "      <td>Moringa School introduces the concept of Cloud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>Why is Cloud Computing considered essential in...</td>\n",
       "      <td>Cloud Computing is considered essential in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>Can you provide an overview of the Cloud Compu...</td>\n",
       "      <td>The Cloud Computing program at Moringa School ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>What are the eligibility requirements for the ...</td>\n",
       "      <td>To be eligible for the Cloud Computing program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>How long do students need to dedicate to succe...</td>\n",
       "      <td>Students are required to dedicate 3 days to le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>What unique learning experiences does Moringa ...</td>\n",
       "      <td>Moringa School offers a unique learning experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>How is the curriculum at Moringa School tailor...</td>\n",
       "      <td>The curriculum at Moringa School is tailored t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>What specific areas does the Cloud Computing c...</td>\n",
       "      <td>The Cloud Computing course covers various area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>In what ways does Moringa School guarantee a c...</td>\n",
       "      <td>Moringa School guarantees a comprehensive unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>What is the AWS Cloud course at Moringa School...</td>\n",
       "      <td>The AWS Cloud course at Moringa School is desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>Can you provide an overview of the topics cove...</td>\n",
       "      <td>The course covers a range of topics, including...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>How long does the AWS Cloud course at Moringa ...</td>\n",
       "      <td>The AWS Cloud course typically lasts for a 3 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>What are the eligibility criteria for enrollin...</td>\n",
       "      <td>Eligibility criteria for enrolling in the AWS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>Is the AWS Cloud course suitable for both tech...</td>\n",
       "      <td>AWS Cloud course is suitable for both technica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>What specific skills and knowledge will partic...</td>\n",
       "      <td>Participants in the AWS Cloud course will gain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>Are there hands-on practical components in the...</td>\n",
       "      <td>Absolutely, the course incorporates hands-on p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>Can you explain the structure of the AWS Cloud...</td>\n",
       "      <td>The curriculum of the AWS Cloud course is stru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>What kind of support do students receive durin...</td>\n",
       "      <td>Students in the AWS Cloud course receive suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>aws_cloud_course</td>\n",
       "      <td>Are there any prerequisites or recommended pre...</td>\n",
       "      <td>While there may not be strict prerequisites, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tag                                           question  \\\n",
       "0    cloud_computing  What is the fundamental starting point for und...   \n",
       "1    cloud_computing  How does Moringa School introduce the concept ...   \n",
       "2    cloud_computing  Why is Cloud Computing considered essential in...   \n",
       "3    cloud_computing  Can you provide an overview of the Cloud Compu...   \n",
       "4    cloud_computing  What are the eligibility requirements for the ...   \n",
       "5    cloud_computing  How long do students need to dedicate to succe...   \n",
       "6    cloud_computing  What unique learning experiences does Moringa ...   \n",
       "7    cloud_computing  How is the curriculum at Moringa School tailor...   \n",
       "8    cloud_computing  What specific areas does the Cloud Computing c...   \n",
       "9    cloud_computing  In what ways does Moringa School guarantee a c...   \n",
       "10  aws_cloud_course  What is the AWS Cloud course at Moringa School...   \n",
       "11  aws_cloud_course  Can you provide an overview of the topics cove...   \n",
       "12  aws_cloud_course  How long does the AWS Cloud course at Moringa ...   \n",
       "13  aws_cloud_course  What are the eligibility criteria for enrollin...   \n",
       "14  aws_cloud_course  Is the AWS Cloud course suitable for both tech...   \n",
       "15  aws_cloud_course  What specific skills and knowledge will partic...   \n",
       "16  aws_cloud_course  Are there hands-on practical components in the...   \n",
       "17  aws_cloud_course  Can you explain the structure of the AWS Cloud...   \n",
       "18  aws_cloud_course  What kind of support do students receive durin...   \n",
       "19  aws_cloud_course  Are there any prerequisites or recommended pre...   \n",
       "\n",
       "                                             response  \n",
       "0   The fundamental starting point for understandi...  \n",
       "1   Moringa School introduces the concept of Cloud...  \n",
       "2   Cloud Computing is considered essential in the...  \n",
       "3   The Cloud Computing program at Moringa School ...  \n",
       "4   To be eligible for the Cloud Computing program...  \n",
       "5   Students are required to dedicate 3 days to le...  \n",
       "6   Moringa School offers a unique learning experi...  \n",
       "7   The curriculum at Moringa School is tailored t...  \n",
       "8   The Cloud Computing course covers various area...  \n",
       "9   Moringa School guarantees a comprehensive unde...  \n",
       "10  The AWS Cloud course at Moringa School is desi...  \n",
       "11  The course covers a range of topics, including...  \n",
       "12  The AWS Cloud course typically lasts for a 3 d...  \n",
       "13  Eligibility criteria for enrolling in the AWS ...  \n",
       "14  AWS Cloud course is suitable for both technica...  \n",
       "15  Participants in the AWS Cloud course will gain...  \n",
       "16  Absolutely, the course incorporates hands-on p...  \n",
       "17  The curriculum of the AWS Cloud course is stru...  \n",
       "18  Students in the AWS Cloud course receive suppo...  \n",
       "19  While there may not be strict prerequisites, a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(paired_data)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8860a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 599 entries, 0 to 598\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tag       599 non-null    object\n",
      " 1   question  599 non-null    object\n",
      " 2   response  599 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 14.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18949925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a3cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to questions and responses\n",
    "df['question'] = df['question'].apply(preprocess_text)\n",
    "df['response'] = df['response'].apply(preprocess_text)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['question'].tolist() + train_data['response'].tolist())\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_questions_seq = tokenizer.texts_to_sequences(train_data['question'].tolist())\n",
    "train_responses_seq = tokenizer.texts_to_sequences(train_data['response'].tolist())\n",
    "\n",
    "# Finding the maximum sequence length\n",
    "max_seq_length = max(max(len(seq) for seq in train_questions_seq), max(len(seq) for seq in train_responses_seq))\n",
    "\n",
    "# Padding sequences\n",
    "train_questions_padded = pad_sequences(train_questions_seq, maxlen=max_seq_length, padding='post')\n",
    "train_responses_padded = pad_sequences(train_responses_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Preparing test data\n",
    "test_questions_seq = tokenizer.texts_to_sequences(test_data['question'].tolist())\n",
    "test_responses_seq = tokenizer.texts_to_sequences(test_data['response'].tolist())\n",
    "test_questions_padded = pad_sequences(test_questions_seq, maxlen=max_seq_length, padding='post')\n",
    "test_responses_padded = pad_sequences(test_responses_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# One-hot encode the responses\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "train_responses_one_hot = np.zeros((len(train_responses_padded), max_seq_length, vocab_size))\n",
    "for i, sequence in enumerate(train_responses_padded):\n",
    "    for j, word_index in enumerate(sequence):\n",
    "        train_responses_one_hot[i, j, word_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21b2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, Attention, Concatenate\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a54890",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([train_questions_padded, train_responses_padded], train_responses_one_hot,\n",
    "                    batch_size=64,\n",
    "                    epochs=20,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c08c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the test responses\n",
    "test_responses_one_hot = np.zeros((len(test_responses_padded), max_seq_length, vocab_size))\n",
    "for i, sequence in enumerate(test_responses_padded):\n",
    "    for j, word_index in enumerate(sequence):\n",
    "        test_responses_one_hot[i, j, word_index] = 1\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate([test_questions_padded, test_responses_padded], test_responses_one_hot)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = model.evaluate([train_questions_padded, train_responses_padded], train_responses_one_hot)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe300b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_accuracy = model.evaluate([train_questions_padded, train_responses_padded], train_responses_one_hot)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae28cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_text(sequence, tokenizer):\n",
    "    # Convert a sequence of indices back to text\n",
    "    return \" \".join([word for word, index in tokenizer.word_index.items() if index in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfefb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict([test_questions_padded, test_responses_padded])\n",
    "\n",
    "# Convert one-hot encoded predictions back to text\n",
    "predicted_texts = []\n",
    "actual_texts = []\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    predicted_sequence = np.argmax(test_predictions[i], axis=-1)\n",
    "    predicted_text = seq_to_text(predicted_sequence, tokenizer)\n",
    "    actual_text = seq_to_text(test_responses_padded[i], tokenizer)\n",
    "    \n",
    "    predicted_texts.append(predicted_text)\n",
    "    actual_texts.append(actual_text)\n",
    "\n",
    "# Now you can calculate BLEU and ROUGE scores\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge \n",
    "\n",
    "# BLEU Score\n",
    "hypothesis = [text.split() for text in predicted_texts]\n",
    "references = [[text.split()] for text in actual_texts]\n",
    "bleu_score = corpus_bleu(references, hypothesis)\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "# ROUGE Score\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(predicted_texts, actual_texts, avg=True)\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de372873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
