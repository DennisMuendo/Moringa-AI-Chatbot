{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153c69f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7f227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609bba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('../Final_Intents.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9a65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pair each question with its corresponding response\n",
    "def pair_questions_responses(data):\n",
    "    paired_data = []\n",
    "    for item in data:\n",
    "        tag = item.get('tag', 'Unknown')\n",
    "        questions = item.get('questions', [])\n",
    "        responses = item.get('responses', [])\n",
    "\n",
    "        for question, response in zip(questions, responses):\n",
    "            paired_data.append({'tag': tag, 'question': question, 'response': response})\n",
    "\n",
    "    return paired_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbf95b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>What is the fundamental starting point for und...</td>\n",
       "      <td>The fundamental starting point for understandi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>How does Moringa School introduce the concept ...</td>\n",
       "      <td>Moringa School introduces the concept of Cloud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>Why is Cloud Computing considered essential in...</td>\n",
       "      <td>Cloud Computing is considered essential in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>Can you provide an overview of the Cloud Compu...</td>\n",
       "      <td>The Cloud Computing program at Moringa School ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cloud_computing</td>\n",
       "      <td>What are the eligibility requirements for the ...</td>\n",
       "      <td>To be eligible for the Cloud Computing program...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag                                           question  \\\n",
       "0  cloud_computing  What is the fundamental starting point for und...   \n",
       "1  cloud_computing  How does Moringa School introduce the concept ...   \n",
       "2  cloud_computing  Why is Cloud Computing considered essential in...   \n",
       "3  cloud_computing  Can you provide an overview of the Cloud Compu...   \n",
       "4  cloud_computing  What are the eligibility requirements for the ...   \n",
       "\n",
       "                                            response  \n",
       "0  The fundamental starting point for understandi...  \n",
       "1  Moringa School introduces the concept of Cloud...  \n",
       "2  Cloud Computing is considered essential in the...  \n",
       "3  The Cloud Computing program at Moringa School ...  \n",
       "4  To be eligible for the Cloud Computing program...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_data = pair_questions_responses(data)\n",
    "df = pd.DataFrame(paired_data)\n",
    "df.head ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67334f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to questions and responses\n",
    "df['question'] = df['question'].apply(preprocess_text)\n",
    "df['response'] = df['response'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf5a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f68c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['question'].tolist() + train_data['response'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dda5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequences\n",
    "train_questions_seq = tokenizer.texts_to_sequences(train_data['question'].tolist())\n",
    "train_responses_seq = tokenizer.texts_to_sequences(train_data['response'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d87aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the maximum sequence length\n",
    "max_seq_length = max(max(len(seq) for seq in train_questions_seq), max(len(seq) for seq in train_responses_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8314670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "train_questions_padded = pad_sequences(train_questions_seq, maxlen=max_seq_length, padding='post')\n",
    "train_responses_padded = pad_sequences(train_responses_seq, maxlen=max_seq_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a762cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "test_questions_seq = tokenizer.texts_to_sequences(test_data['question'].tolist())\n",
    "test_responses_seq = tokenizer.texts_to_sequences(test_data['response'].tolist())\n",
    "test_questions_padded = pad_sequences(test_questions_seq, maxlen=max_seq_length, padding='post')\n",
    "test_responses_padded = pad_sequences(test_responses_seq, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b43050b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the responses\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "train_responses_one_hot = np.zeros((len(train_responses_padded), max_seq_length, vocab_size))\n",
    "\n",
    "for i, sequence in enumerate(train_responses_padded):\n",
    "    for j, word_index in enumerate(sequence):\n",
    "        train_responses_one_hot[i, j, word_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bb18d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb0a9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Updated Model Architecture\n",
    "embedding_dim = 128  # Embedding dimensionality\n",
    "latent_dim = 256 \n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_seq_length,))\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Bahdanau Attention\n",
    "attention_layer = BahdanauAttention(latent_dim)\n",
    "context_vector, attention_weights = attention_layer(state_h, encoder_outputs)\n",
    "\n",
    "# Decoder with attention\n",
    "decoder_inputs = Input(shape=(max_seq_length,))\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "context_vector_with_time_axis = tf.expand_dims(context_vector, 1)\n",
    "\n",
    "# Broadcast the context vector to have the same shape as the decoder embedding\n",
    "sequence_length = tf.shape(decoder_embedding)[1]\n",
    "context_vector_broadcasted = tf.broadcast_to(context_vector_with_time_axis, [tf.shape(context_vector_with_time_axis)[0], sequence_length, tf.shape(context_vector_with_time_axis)[-1]])\n",
    "decoder_input_combined = tf.concat([context_vector_broadcasted, decoder_embedding], axis=-1)\n",
    "\n",
    "# Continue with the decoder LSTM, etc.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_input_combined, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "266ea668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model with Adam optimizer and use categorical_crossentropy as loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c824a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 148)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 148, 128)             189696    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 256),                394240    ['embedding[0][0]']           \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " bahdanau_attention (Bahdan  ((None, 256),                131841    ['lstm[0][1]',                \n",
      " auAttention)                 (None, None, 1))                       'lstm[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 148)]                0         []                            \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1, 256)               0         ['bahdanau_attention[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 148, 128)             189696    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOp  (3,)                         0         ['tf.expand_dims[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLa  (3,)                         0         ['embedding_1[0][0]']         \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_2 (TFOp  (3,)                         0         ['tf.expand_dims[0][0]']      \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_1[0][0]']\n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  ()                           0         ['tf.compat.v1.shape[0][0]']  \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  ()                           0         ['tf.compat.v1.shape_2[0][0]']\n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.broadcast_to (TFOpLambd  (None, 148, 256)             0         ['tf.expand_dims[0][0]',      \n",
      " a)                                                                  'tf.__operators__.getitem_1[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'tf.__operators__.getitem[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'tf.__operators__.getitem_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)      (None, 148, 384)             0         ['tf.broadcast_to[0][0]',     \n",
      "                                                                     'embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 148, 256),           656384    ['tf.concat[0][0]',           \n",
      "                              (None, 256),                           'lstm[0][1]',                \n",
      "                              (None, 256)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 148, 1482)            380874    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1942731 (7.41 MB)\n",
      "Trainable params: 1942731 (7.41 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c098d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "6/6 [==============================] - 21s 2s/step - loss: 6.3879 - accuracy: 0.7488 - val_loss: 3.2817 - val_accuracy: 0.9045\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 13s 2s/step - loss: 1.9465 - accuracy: 0.8998 - val_loss: 0.8621 - val_accuracy: 0.9053\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.8309 - accuracy: 0.9005 - val_loss: 0.7362 - val_accuracy: 0.9060\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7656 - accuracy: 0.9003 - val_loss: 0.6933 - val_accuracy: 0.9061\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7411 - accuracy: 0.9005 - val_loss: 0.6801 - val_accuracy: 0.9053\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7277 - accuracy: 0.9004 - val_loss: 0.6713 - val_accuracy: 0.9055\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7225 - accuracy: 0.9006 - val_loss: 0.6711 - val_accuracy: 0.9057\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 15s 2s/step - loss: 0.7180 - accuracy: 0.9006 - val_loss: 0.6694 - val_accuracy: 0.9055\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 15s 3s/step - loss: 0.7137 - accuracy: 0.9007 - val_loss: 0.6707 - val_accuracy: 0.9053\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7211 - accuracy: 0.9005 - val_loss: 0.6786 - val_accuracy: 0.9053\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7539 - accuracy: 0.9005 - val_loss: 0.7219 - val_accuracy: 0.9055\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7936 - accuracy: 0.9004 - val_loss: 0.7690 - val_accuracy: 0.9054\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.8232 - accuracy: 0.9004 - val_loss: 0.8189 - val_accuracy: 0.9053\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.8527 - accuracy: 0.9001 - val_loss: 0.8458 - val_accuracy: 0.9051\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.8664 - accuracy: 0.9000 - val_loss: 0.8271 - val_accuracy: 0.9053\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 15s 2s/step - loss: 0.8503 - accuracy: 0.8999 - val_loss: 0.7979 - val_accuracy: 0.9051\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.8094 - accuracy: 0.8999 - val_loss: 0.7682 - val_accuracy: 0.9049\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7819 - accuracy: 0.8998 - val_loss: 0.7478 - val_accuracy: 0.9049\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7618 - accuracy: 0.8998 - val_loss: 0.7281 - val_accuracy: 0.9050\n",
      "Epoch 20/20\n",
      "6/6 [==============================] - 14s 2s/step - loss: 0.7412 - accuracy: 0.8998 - val_loss: 0.7139 - val_accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit([train_questions_padded, train_responses_padded], train_responses_one_hot,\n",
    "                    batch_size=64,\n",
    "                    epochs=20,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a08ce571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 312ms/step - loss: 0.6787 - accuracy: 0.9083\n",
      "Test Loss: 0.6787163615226746\n",
      "Test Accuracy: 0.9083333611488342\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_responses_one_hot = np.zeros((len(test_responses_padded), max_seq_length, vocab_size))\n",
    "\n",
    "for i, sequence in enumerate(test_responses_padded):\n",
    "    for j, word_index in enumerate(sequence):\n",
    "        test_responses_one_hot[i, j, word_index] = 1\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate([test_questions_padded, test_responses_padded], test_responses_one_hot)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6c22389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 5s 332ms/step - loss: 0.7295 - accuracy: 0.9008\n",
      "Train Loss: 0.7295042276382446\n",
      "Train Accuracy: 0.9007927775382996\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = model.evaluate([train_questions_padded, train_responses_padded], train_responses_one_hot)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e18511b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_text(sequence, tokenizer):\n",
    "    # Convert a sequence of indices back to text\n",
    "    return \" \".join([word for word, index in tokenizer.word_index.items() if index in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a0f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
