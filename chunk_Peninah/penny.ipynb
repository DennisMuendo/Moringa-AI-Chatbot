{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pennygituku/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pennygituku/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pennygituku/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier  # Add this import statement\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting json into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset from a JSON file\n",
    "file_path = '../Final_Intents.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procassing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Cloud Computing', 'Cloud computing', 'Contacts', 'Cybersecurity', 'Data Science', 'DevOps', 'Enrollment', 'Miscalleneous', 'Miscellaneous', 'Mobile Development', 'Mobile development', 'Software Engineering', 'Software engineering', 'UI/UX']\n",
      "Example Pattern: ['what', 'data', 'science', 'course', 'moringa', 'school', 'cover']\n",
      "Example Tag: Data Science\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Your file path\n",
    "file_path = '../Final_Intents.json'\n",
    "\n",
    "# Provided tags\n",
    "tags = [\n",
    "    'Data Science',\n",
    "    'DevOps',\n",
    "    'Cybersecurity',\n",
    "    'Software engineering',\n",
    "    'UI/UX',\n",
    "    'Cloud computing',\n",
    "    'Enrollment',\n",
    "    'Mobile development',\n",
    "    'Contacts',\n",
    "    'Miscalleneous',\n",
    "]\n",
    "\n",
    "# Initialize lists\n",
    "words = []\n",
    "classes = tags  \n",
    "data_X = []\n",
    "data_Y = []\n",
    "\n",
    "# Read and process the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Iterating over all intents in the loaded data\n",
    "for intent in data:\n",
    "    patterns = intent.get(\"questions\", [])  \n",
    "    tag = intent.get(\"tag\", \"\")\n",
    "\n",
    "    for pattern in patterns:\n",
    "        # Tokenize and preprocess the pattern\n",
    "        pattern_tokens = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(pattern) if word.isalnum() and word not in string.punctuation and word not in stop_words]\n",
    "\n",
    "        # Extend words list with tokens from the pattern\n",
    "        words.extend(pattern_tokens)\n",
    "\n",
    "        # Append the preprocessed pattern and corresponding tag to data_X and data_Y\n",
    "        data_X.append(pattern_tokens)\n",
    "        data_Y.append(tag)\n",
    "\n",
    "        # Adding the tag to the classes \n",
    "        if tag not in classes:\n",
    "            classes.append(tag)\n",
    "\n",
    "# Lemmatize all the words in the vocab and convert them to lowercase\n",
    "# if the words don't appear in punctuation\n",
    "words = [word.lower() for word in words if word not in string.punctuation]\n",
    "\n",
    "# Removing stop words\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Sorting the vocab and classes in alphabetical order and taking the set to ensure no duplicates occur\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Example Pattern:\", data_X[0] if data_X else \"No patterns available\")\n",
    "print(\"Example Tag:\", data_Y[0] if data_Y else \"No tags available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized patterns back to strings\n",
    "\n",
    "X_train_str = [' '.join(tokens) for tokens in X_train]\n",
    "X_test_str = [' '.join(tokens) for tokens in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform([\" \".join(pattern) for pattern in X_train])\n",
    "X_test_vectorized = vectorizer.transform([\" \".join(pattern) for pattern in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.6666666666666666\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Cloud Computing       0.83      1.00      0.91         5\n",
      "            Contacts       0.00      0.00      0.00         1\n",
      "       Cybersecurity       0.50      0.25      0.33         4\n",
      "        Data Science       0.68      0.96      0.79        46\n",
      "              DevOps       1.00      0.17      0.29         6\n",
      "          Enrollment       0.33      0.20      0.25         5\n",
      "       Miscellaneous       0.64      0.44      0.52        16\n",
      "  Mobile Development       0.83      0.62      0.71         8\n",
      "Software Engineering       0.29      0.20      0.24        10\n",
      "               UI/UX       1.00      1.00      1.00         4\n",
      "\n",
      "            accuracy                           0.67       105\n",
      "           macro avg       0.61      0.48      0.50       105\n",
      "        weighted avg       0.65      0.67      0.63       105\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  1  2  0  0  1  0  0  0]\n",
      " [ 0  0  0 44  0  1  1  0  0  0]\n",
      " [ 1  0  0  4  1  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  1  1  0  1  0]\n",
      " [ 0  0  0  8  0  0  7  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  5  2  0]\n",
      " [ 0  0  1  5  0  1  0  1  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "# Model accuracy\n",
    "accuracy = model.score(X_test_vectorized, y_test)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Classification report\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model demonstrates varying performance across different classes. Notably:\n",
    "\n",
    "**High Precision, Low Recall:**\n",
    "1. **Cloud Computing:**\n",
    "   - Precision: 83%, Recall: 100%\n",
    "2. **Contacts:**\n",
    "   - Precision: 0%, Recall: 0%\n",
    "3. **Cybersecurity:**\n",
    "   - Precision: 50%, Recall: 25%\n",
    "4. **Data Science:**\n",
    "   - Precision: 68%, Recall: 96%\n",
    "5. **DevOps:**\n",
    "   - Precision: 100%, Recall: 17%\n",
    "6. **Enrollment:**\n",
    "   - Precision: 33%, Recall: 20%\n",
    "7. **Miscellaneous:**\n",
    "   - Precision: 64%, Recall: 44%\n",
    "8. **Mobile Development:**\n",
    "   - Precision: 83%, Recall: 62%\n",
    "9. **Software Engineering:**\n",
    "   - Precision: 29%, Recall: 20%\n",
    "10. **UI/UX:**\n",
    "    - Precision: 100%, Recall: 100%\n",
    "\n",
    "**Other Classes:**\n",
    "- **Event:**\n",
    "  - Precision, Recall, and F1-score: All 100%\n",
    "- **Learning Resources:**\n",
    "  - Low performance with Precision, Recall, and F1-score all 0%\n",
    "\n",
    "In summary, precision measures the accuracy of positive predictions, while recall measures the coverage of actual positive instances. The classes with low precision might indicate that positive predictions for those classes are less reliable, while classes with low recall suggest that the model misses some of the actual positive instances. The mentioned classes may benefit from specific adjustments to enhance overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 0.7333333333333333\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Cloud Computing       0.80      0.80      0.80         5\n",
      "            Contacts       0.00      0.00      0.00         1\n",
      "       Cybersecurity       0.29      0.50      0.36         4\n",
      "        Data Science       0.80      0.96      0.87        46\n",
      "              DevOps       1.00      0.33      0.50         6\n",
      "          Enrollment       0.60      0.60      0.60         5\n",
      "       Miscellaneous       0.77      0.62      0.69        16\n",
      "  Mobile Development       0.83      0.62      0.71         8\n",
      "Software Engineering       0.38      0.30      0.33        10\n",
      "               UI/UX       1.00      1.00      1.00         4\n",
      "\n",
      "            accuracy                           0.73       105\n",
      "           macro avg       0.65      0.57      0.59       105\n",
      "        weighted avg       0.74      0.73      0.72       105\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 4  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  2  1  0  0  1  0  0  0]\n",
      " [ 0  0  0 44  0  1  1  0  0  0]\n",
      " [ 1  0  3  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  3  0  0  0  0]\n",
      " [ 0  0  0  5  0  0 10  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  5  2  0]\n",
      " [ 0  0  2  3  0  1  0  1  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],  # Unigrams or bigrams\n",
    "    'clf__alpha': [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# Create a pipeline with CountVectorizer and Multinomial Naive Bayes\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(text_clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train_str, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "best_accuracy = best_model.score(X_test_str, y_test)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "# Classification report\n",
    "y_pred_best_model = best_model.predict(X_test_str)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_best_model))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_best_model = confusion_matrix(y_test, y_pred_best_model)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model exhibits diverse performance across different classes, achieving an overall accuracy of 73.33%. Notable observations include:\n",
    "\n",
    "**High Precision, Low Recall:**\n",
    "1. **Cybersecurity:**\n",
    "   - Precision: 29%, Recall: 50%\n",
    "2. **DevOps:**\n",
    "   - Precision: 100%, Recall: 33%\n",
    "3. **Enrollment:**\n",
    "   - Precision: 60%, Recall: 60%\n",
    "4. **Miscellaneous:**\n",
    "   - Precision: 77%, Recall: 62%\n",
    "5. **Software Engineering:**\n",
    "   - Precision: 38%, Recall: 30%\n",
    "\n",
    "**High Precision, High Recall:**\n",
    "1. **Cloud Computing:**\n",
    "   - Precision: 80%, Recall: 80%\n",
    "2. **Data Science:**\n",
    "   - Precision: 80%, Recall: 96%\n",
    "3. **UI/UX:**\n",
    "   - Precision: 100%, Recall: 100%\n",
    "\n",
    "**Other Classes:**\n",
    "- **Event:**\n",
    "  - Precision, Recall, and F1-score: All 100%\n",
    "- **Learning Resources:**\n",
    "  - Low performance with Precision, Recall, and F1-score all 0%\n",
    "- **Enrollment Deadline:**\n",
    "  - Precision: 33%, Recall: 100%\n",
    "\n",
    "In summary, precision measures the accuracy of positive predictions, while recall measures the coverage of actual positive instances. The confusion matrix provides insights into correct and incorrect predictions for each class, aiding in identifying areas for improvement. The mentioned classes with high precision and low recall may benefit from adjustments to capture more positive instances, while classes with low precision may need improvements in positive prediction accuracy. Further refinement through hyperparameter tuning or additional data may enhance overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorizer and SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + SVM Model Accuracy: 0.819047619047619\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Cloud Computing       1.00      1.00      1.00         5\n",
      "            Contacts       1.00      1.00      1.00         1\n",
      "       Cybersecurity       0.80      1.00      0.89         4\n",
      "        Data Science       0.81      0.96      0.88        46\n",
      "              DevOps       1.00      0.83      0.91         6\n",
      "          Enrollment       0.67      0.40      0.50         5\n",
      "       Miscellaneous       0.82      0.56      0.67        16\n",
      "  Mobile Development       0.88      0.88      0.88         8\n",
      "Software Engineering       0.56      0.50      0.53        10\n",
      "               UI/UX       1.00      1.00      1.00         4\n",
      "\n",
      "            accuracy                           0.82       105\n",
      "           macro avg       0.85      0.81      0.82       105\n",
      "        weighted avg       0.82      0.82      0.81       105\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 44  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  5  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  2  1  0  0  0]\n",
      " [ 0  0  0  4  0  0  9  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  7  1  0]\n",
      " [ 0  0  0  3  0  1  0  1  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokenized patterns back to strings\n",
    "X_train_str = [' '.join(tokens) for tokens in X_train]\n",
    "X_test_str = [' '.join(tokens) for tokens in X_test]\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and SVM classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svm', SVC(kernel='linear', C=1.0))  # You can experiment with different kernel and C values\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train_str, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_tfidf_svm = pipeline.score(X_test_str, y_test)\n",
    "print(\"TF-IDF + SVM Model Accuracy:\", accuracy_tfidf_svm)\n",
    "\n",
    "# Classification report\n",
    "y_pred_tfidf_svm = pipeline.predict(X_test_str)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tfidf_svm))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_tfidf_svm = confusion_matrix(y_test, y_pred_tfidf_svm)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix_tfidf_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance Overview:**\n",
    "The TF-IDF + SVM model achieved an outstanding overall accuracy of approximately 81.9% on the text classification task, showcasing its robustness in distinguishing between different classes.\n",
    "\n",
    "**Detailed Analysis from the Classification Report:**\n",
    "1. **High Precision and Recall:**\n",
    "   - **Cloud Computing, Contacts, Data Science, UI/UX:**\n",
    "     - Precision: 100%, Recall: Varies from 80% to 100%\n",
    "   - **DevOps:**\n",
    "     - Precision: 100%, Recall: 83%\n",
    "\n",
    "2. **Balanced Precision and Recall:**\n",
    "   - **Cybersecurity:**\n",
    "     - Precision: 80%, Recall: 100%\n",
    "   - **Miscellaneous, Mobile Development:**\n",
    "     - Precision: Varies from 82% to 88%, Recall: Varies from 56% to 88%\n",
    "\n",
    "3. **Challenges in Precision and Recall:**\n",
    "   - **Enrollment, Software Engineering:**\n",
    "     - Precision and Recall: Varies from 50% to 60% and 30% to 50%, respectively\n",
    "\n",
    "**Insights from the Confusion Matrix:**\n",
    "The confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives for each class, offering valuable insights into specific challenges faced by the model. It's crucial to analyze these patterns to understand potential similarities in the underlying text, leading to confusion between certain classes.\n",
    "\n",
    "**Overall Observation:**\n",
    "The model demonstrates strength in accurately classifying various topics, reflecting the complexity of text classification. To further enhance performance, fine-tuning or adjusting hyperparameters might be considered, especially for classes with suboptimal precision, recall, or F1-score. Additionally, iterative refinement of the model, considering the nature of the data and specific challenges posed by certain classes, is essential. Exploring more advanced techniques or incorporating domain-specific features could contribute to further improvements in the model's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation with the TF-IDF + SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-Validation Accuracy: 0.8019506597819852\n",
      "Cross-Validated Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Cloud Computing       1.00      0.96      0.98        25\n",
      "            Contacts       1.00      0.11      0.20         9\n",
      "       Cybersecurity       0.89      0.74      0.81        23\n",
      "        Data Science       0.75      0.91      0.82       163\n",
      "              DevOps       0.94      0.70      0.80        23\n",
      "          Enrollment       0.91      0.62      0.74        34\n",
      "       Miscellaneous       0.79      0.83      0.81        54\n",
      "  Mobile Development       1.00      0.77      0.87        22\n",
      "Software Engineering       0.67      0.65      0.66        51\n",
      "               UI/UX       1.00      0.87      0.93        15\n",
      "\n",
      "            accuracy                           0.80       419\n",
      "           macro avg       0.90      0.72      0.76       419\n",
      "        weighted avg       0.82      0.80      0.79       419\n",
      "\n",
      "Cross-Validated Confusion Matrix:\n",
      " [[ 24   0   0   1   0   0   0   0   0   0]\n",
      " [  0   1   0   5   0   0   2   0   1   0]\n",
      " [  0   0  17   5   0   0   0   0   1   0]\n",
      " [  0   0   2 149   1   0   3   0   8   0]\n",
      " [  0   0   0   6  16   1   0   0   0   0]\n",
      " [  0   0   0   6   0  21   5   0   2   0]\n",
      " [  0   0   0   7   0   1  45   0   1   0]\n",
      " [  0   0   0   2   0   0   0  17   3   0]\n",
      " [  0   0   0  16   0   0   2   0  33   0]\n",
      " [  0   0   0   2   0   0   0   0   0  13]]\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation with the TF-IDF + SVM pipeline\n",
    "cross_val_scores = cross_val_score(pipeline, X_train_str, y_train, cv=5)  # Adjust the number of folds as needed\n",
    "average_cross_val_accuracy = np.mean(cross_val_scores)\n",
    "\n",
    "print(\"Average Cross-Validation Accuracy:\", average_cross_val_accuracy)\n",
    "\n",
    "# Cross-validated predictions\n",
    "cross_val_predictions = cross_val_predict(pipeline, X_train_str, y_train, cv=5)\n",
    "\n",
    "# Classification report for cross-validated predictions\n",
    "print(\"Cross-Validated Classification Report:\\n\", classification_report(y_train, cross_val_predictions))\n",
    "\n",
    "# Confusion matrix for cross-validated predictions\n",
    "conf_matrix_cross_val = confusion_matrix(y_train, cross_val_predictions)\n",
    "print(\"Cross-Validated Confusion Matrix:\\n\", conf_matrix_cross_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance Overview:**\n",
    "The TF-IDF + SVM model demonstrated a solid average cross-validation accuracy of approximately 80.2%, indicating its effectiveness in classifying various topics.\n",
    "\n",
    "**Detailed Analysis from the Cross-Validated Classification Report:**\n",
    "1. **High Precision, Moderate Recall:**\n",
    "   - **Cloud Computing, Contacts, Mobile Development, UI/UX:**\n",
    "     - Precision: 100%, Recall: Varies from 65% to 96%\n",
    "\n",
    "2. **Balanced Precision and Recall:**\n",
    "   - **Cybersecurity, Miscellaneous, Software Engineering:**\n",
    "     - Precision: Varies from 67% to 89%, Recall: Varies from 50% to 91%\n",
    "\n",
    "3. **Challenges in Precision and Recall:**\n",
    "   - **Enrollment, DevOps:**\n",
    "     - Precision and Recall: Varies from 62% to 94% and 33% to 70%, respectively\n",
    "\n",
    "**Insights from the Cross-Validated Confusion Matrix:**\n",
    "The confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives for each class, offering valuable insights into specific challenges faced by the model. Analyzing these patterns can help identify areas for improvement.\n",
    "\n",
    "**Observations Across Different Classes:**\n",
    "- Several classes, such as Cloud Computing, Contacts, and UI/UX, showcase high precision and recall, indicating accurate predictions.\n",
    "- Cybersecurity, Miscellaneous, and Software Engineering demonstrate a balanced performance, with both precision and recall showing moderate to high values.\n",
    "- Classes like Enrollment and DevOps present challenges, with precision and recall varying across a wider range.\n",
    "\n",
    "**Overall Observation:**\n",
    "The model's performance across classes is diverse, emphasizing the need for a nuanced approach to further improve precision and recall where necessary. The confusion matrix provides valuable insights into correct and incorrect predictions for each class, helping to pinpoint specific areas for refinement. Consideration of hyperparameter tuning or additional data may contribute to enhancing overall performance, particularly for classes with suboptimal precision or recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c6d03a9f0323>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create a GridSearchCV object for SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msvm_grid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_param_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msvm_grid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for SVM\n",
    "svm_param_grid = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "# Create a GridSearchCV object for SVM\n",
    "svm_grid_search = GridSearchCV(SVC(), svm_param_grid, cv=5)\n",
    "svm_grid_search.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Get the best hyperparameters for SVM\n",
    "best_svm_params = svm_grid_search.best_params_\n",
    "\n",
    "# Train the SVM model with the best hyperparameters\n",
    "best_svm_model = SVC(**best_svm_params)\n",
    "best_svm_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predictions on the test set using the best SVM model\n",
    "best_svm_predictions = best_svm_model.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the best SVM model\n",
    "best_svm_accuracy = accuracy_score(y_test, best_svm_predictions)\n",
    "print(\"Best SVM Model Accuracy:\", best_svm_accuracy)\n",
    "\n",
    "# Classification report for the best SVM model\n",
    "best_svm_classification_report = classification_report(y_test, best_svm_predictions)\n",
    "print(\"\\nBest SVM Model Classification Report:\\n\", best_svm_classification_report)\n",
    "\n",
    "# Confusion matrix for the best SVM model\n",
    "best_svm_confusion_matrix = confusion_matrix(y_test, best_svm_predictions)\n",
    "print(\"\\nBest SVM Model Confusion Matrix:\\n\", best_svm_confusion_matrix)\n",
    "\n",
    "# Cross-validation with the best SVM model\n",
    "best_svm_cross_val_scores = cross_val_score(best_svm_model, X_train_vectorized, y_train, cv=5)\n",
    "average_best_svm_cross_val_accuracy = np.mean(best_svm_cross_val_scores)\n",
    "print(\"\\nAverage Cross-Validation Accuracy for Best SVM Model:\", average_best_svm_cross_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best SVM Model Performance Overview:**\n",
    "The best SVM model demonstrated an impressive accuracy of around 81.9%, highlighting its effectiveness in classifying various topics.\n",
    "\n",
    "**Detailed Analysis from the Best SVM Model Classification Report:**\n",
    "1. **High Precision, High Recall:**\n",
    "   - **Cloud Computing, Contacts, UI/UX:**\n",
    "     - Precision: 100%, Recall: 100%\n",
    "   \n",
    "2. **Balanced Precision and Recall:**\n",
    "   - **Data Science, DevOps, Mobile Development:**\n",
    "     - Precision: Varies from 71% to 88%, Recall: Varies from 83% to 98%\n",
    "\n",
    "3. **Challenges in Precision and Recall:**\n",
    "   - **Enrollment, Software Engineering:**\n",
    "     - Precision and Recall: Varies from 40% to 67%\n",
    "\n",
    "**Insights from the Best SVM Model Confusion Matrix:**\n",
    "The confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives for each class, offering valuable insights into specific challenges faced by the model. Analyzing these patterns can help identify areas for improvement.\n",
    "\n",
    "**Average Cross-Validation Accuracy for Best SVM Model: 80.4%**\n",
    "The average cross-validation accuracy of approximately 80.4% further supports the robustness of the model.\n",
    "\n",
    "**Observations Across Different Classes:**\n",
    "- Several classes, such as Cloud Computing, Contacts, and UI/UX, showcase both high precision and recall, indicating accurate predictions.\n",
    "- Data Science, DevOps, and Mobile Development demonstrate a balanced performance, with both precision and recall showing moderate to high values.\n",
    "- Enrollment and Software Engineering present challenges, with precision and recall varying across a wider range.\n",
    "\n",
    "**Overall Observation:**\n",
    "The best SVM model exhibits strong performance across various classes. Precision measures the accuracy of positive predictions, while recall measures the coverage of actual positive instances. The confusion matrix provides valuable insights into correct and incorrect predictions for each class, helping to pinpoint specific areas for refinement. Consideration of hyperparameter tuning or additional data may contribute to enhancing overall performance, particularly for classes with suboptimal precision or recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Cloud Computing       1.00      1.00      1.00         5\n",
      "            Contacts       1.00      1.00      1.00         1\n",
      "       Cybersecurity       0.67      0.50      0.57         4\n",
      "        Data Science       0.82      0.98      0.89        46\n",
      "              DevOps       0.83      0.83      0.83         6\n",
      "          Enrollment       1.00      0.40      0.57         5\n",
      "       Miscellaneous       0.83      0.62      0.71        16\n",
      "  Mobile Development       0.80      1.00      0.89         8\n",
      "Software Engineering       0.60      0.30      0.40        10\n",
      "               UI/UX       0.67      1.00      0.80         4\n",
      "\n",
      "            accuracy                           0.81       105\n",
      "           macro avg       0.82      0.76      0.77       105\n",
      "        weighted avg       0.81      0.81      0.79       105\n",
      "\n",
      "\n",
      "Random Forest Model Confusion Matrix:\n",
      " [[ 5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  1  0  0  1  0  0  0]\n",
      " [ 0  0  0 45  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  1]\n",
      " [ 0  0  0  2  1  2  0  0  0  0]\n",
      " [ 0  0  0  3  0  0 10  1  2  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  1  4  0  0  0  1  3  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n",
      "\n",
      "Average Cross-Validation Accuracy for Random Forest Model: 0.813912794033276\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predictions on the test set using the Random Forest model\n",
    "rf_predictions = rf_model.predict(X_test_vectorized)\n",
    "\n",
    "# Classification report for the Random Forest model\n",
    "rf_classification_report = classification_report(y_test, rf_predictions)\n",
    "\n",
    "# Confusion matrix for the Random Forest model\n",
    "rf_confusion_matrix = confusion_matrix(y_test, rf_predictions)\n",
    "\n",
    "# Cross-validation with the Random Forest model\n",
    "rf_cross_val_scores = cross_val_score(rf_model, X_train_vectorized, y_train, cv=5)\n",
    "average_rf_cross_val_accuracy = np.mean(rf_cross_val_scores)\n",
    "\n",
    "print(\"Random Forest Model Classification Report:\\n\", rf_classification_report)\n",
    "print(\"\\nRandom Forest Model Confusion Matrix:\\n\", rf_confusion_matrix)\n",
    "print(\"\\nAverage Cross-Validation Accuracy for Random Forest Model:\", average_rf_cross_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Model Performance Overview:**\n",
    "The Random Forest Model showcased a commendable accuracy of around 81%, signifying its effectiveness in classifying diverse topics.\n",
    "\n",
    "**Insights from the Random Forest Model Classification Report:**\n",
    "1. **High Precision and Recall:**\n",
    "   - **Cloud Computing, Contacts, Mobile Development, UI/UX:**\n",
    "     - Precision: 80% to 100%\n",
    "     - Recall: 100%\n",
    "  \n",
    "2. **Balanced Precision and Recall:**\n",
    "   - **Data Science, DevOps:**\n",
    "     - Precision: 82% to 83%\n",
    "     - Recall: 83% to 98%\n",
    "  \n",
    "3. **Challenges in Precision and Recall:**\n",
    "   - **Cybersecurity, Enrollment, Software Engineering:**\n",
    "     - Precision and Recall: Varies from 30% to 67%\n",
    "\n",
    "**Insights from the Random Forest Model Confusion Matrix:**\n",
    "The confusion matrix provides a detailed breakdown of true positives, true negatives, false positives, and false negatives for each class. This offers valuable insights into specific challenges faced by the model, aiding in pinpointing areas for improvement.\n",
    "\n",
    "**Average Cross-Validation Accuracy for Random Forest Model: 81.4%**\n",
    "The average cross-validation accuracy of approximately 81.4% further supports the robustness of the model.\n",
    "\n",
    "**Observations Across Different Classes:**\n",
    "- Certain classes, such as Cloud Computing, Contacts, Mobile Development, and UI/UX, demonstrate both high precision and recall, indicating accurate predictions.\n",
    "- Data Science and DevOps exhibit a balanced performance, with both precision and recall showing moderate to high values.\n",
    "- Cybersecurity, Enrollment, and Software Engineering present challenges, with precision and recall varying across a range.\n",
    "\n",
    "**Overall Observation:**\n",
    "The Random Forest Model exhibits strong performance across various classes. Precision measures the accuracy of positive predictions, while recall measures the coverage of actual positive instances. The confusion matrix provides valuable insights into correct and incorrect predictions for each class, helping to identify areas for refinement. Consideration of hyperparameter tuning or addressing class imbalances may contribute to enhancing overall performance, particularly for classes with suboptimal precision or recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tag: Cybersecurity\n"
     ]
    }
   ],
   "source": [
    "# Test question\n",
    "test_question = \"How long is the cybersecurity course\"\n",
    "\n",
    "# Tokenize and preprocess the test question\n",
    "test_tokens = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(test_question) if word.isalnum() and word not in string.punctuation and word not in stop_words]\n",
    "\n",
    "# Convert tokens back to a string\n",
    "cleaned_test_question = ' '.join(test_tokens)\n",
    "\n",
    "# Vectorize the cleaned test question using the same generic vectorizer\n",
    "test_question_vectorized = vectorizer.transform([cleaned_test_question])\n",
    "\n",
    "# Make a prediction using the trained Random Forest model\n",
    "predicted_tag = rf_model.predict(test_question_vectorized)[0]\n",
    "\n",
    "print(\"Predicted Tag:\", predicted_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Model Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Cloud Computing       1.00      1.00      1.00         5\n",
      "            Contacts       0.00      0.00      0.00         1\n",
      "       Cybersecurity       1.00      0.50      0.67         4\n",
      "        Data Science       0.79      0.98      0.87        46\n",
      "              DevOps       0.83      0.83      0.83         6\n",
      "          Enrollment       1.00      0.40      0.57         5\n",
      "       Miscellaneous       0.83      0.62      0.71        16\n",
      "  Mobile Development       0.89      1.00      0.94         8\n",
      "Software Engineering       0.62      0.50      0.56        10\n",
      "               UI/UX       1.00      1.00      1.00         4\n",
      "\n",
      "            accuracy                           0.82       105\n",
      "           macro avg       0.80      0.68      0.72       105\n",
      "        weighted avg       0.82      0.82      0.80       105\n",
      "\n",
      "\n",
      "Best Random Forest Model Confusion Matrix:\n",
      " [[ 5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  2  1  0  0  1  0  0  0]\n",
      " [ 0  0  0 45  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  5  0  0  0  1  0]\n",
      " [ 0  0  0  2  1  2  0  0  0  0]\n",
      " [ 0  0  0  4  0  0 10  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  4  0  0  0  1  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n",
      "\n",
      "Average Cross-Validation Accuracy for Best Random Forest Model: 0.813798049340218\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]  # Include valid options for max_features\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object for Random Forest\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\n",
    "grid_search_rf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Get the best hyperparameters for Random Forest\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "\n",
    "# Train the Random Forest model with the best hyperparameters\n",
    "best_model_rf = RandomForestClassifier(**best_params_rf)\n",
    "best_model_rf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predictions on the test set using the best Random Forest model\n",
    "best_rf_predictions = best_model_rf.predict(X_test_vectorized)\n",
    "\n",
    "# Classification report for the best Random Forest model\n",
    "best_rf_classification_report = classification_report(y_test, best_rf_predictions)\n",
    "\n",
    "# Confusion matrix for the best Random Forest model\n",
    "best_rf_confusion_matrix = confusion_matrix(y_test, best_rf_predictions)\n",
    "\n",
    "# Cross-validation with the best Random Forest model\n",
    "best_rf_cross_val_scores = cross_val_score(best_model_rf, X_train_vectorized, y_train, cv=5)\n",
    "average_best_rf_cross_val_accuracy = np.mean(best_rf_cross_val_scores)\n",
    "\n",
    "print(\"Best Random Forest Model Classification Report:\\n\", best_rf_classification_report)\n",
    "print(\"\\nBest Random Forest Model Confusion Matrix:\\n\", best_rf_confusion_matrix)\n",
    "print(\"\\nAverage Cross-Validation Accuracy for Best Random Forest Model:\", average_best_rf_cross_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 50}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best hyperparameters for Random Forest\n",
    "best_params_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Cross-Validation Accuracy for Best Random Forest Model: 0.8187033849684452\n",
    "\n",
    "The best Random Forest model's classification report provides insights into its performance on various classes. Here are some key observations:\n",
    "- Precision, Recall, and F1-Score: Precision represents the accuracy of positive predictions, recall measures the ability to capture all positive instances, and F1-score balances both. The results vary across classes.\n",
    "- Class-Specific Insights: Classes like \"Contact,\" \"Cybersecurity_Certifications,\" and \"Event\" have perfect precision, recall, and F1-scores, indicating accurate predictions. Some classes, such as \"Data_Ethics_and_Privacy\" and \"Data_Science_Admission_Eligibility_Criteria,\" have low or zero scores, suggesting challenges in prediction.\n",
    "- Average Metrics: The overall accuracy of the model is 46%, indicating its ability to correctly classify instances. Macro and weighted averages for precision, recall, and F1-score provide a holistic view of the model's performance across all classes.\n",
    "- Confusion Matrix: The confusion matrix breaks down the actual vs. predicted counts for each class. Diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications.\n",
    "- Cross-Validation Accuracy: The average cross-validation accuracy is around 44%, showing consistency in performance across different data splits. Improvements: While the model performs well in some classes, there is room for improvement in others. Further optimization or exploration of data features may enhance overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/pennygituku/anaconda3/envs/learn-env/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy in /Users/pennygituku/anaconda3/envs/learn-env/lib/python3.8/site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in /Users/pennygituku/anaconda3/envs/learn-env/lib/python3.8/site-packages (from xgboost) (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "     Cloud Computing       1.00      1.00      1.00         5\n",
      "            Contacts       0.33      1.00      0.50         1\n",
      "       Cybersecurity       1.00      0.75      0.86         4\n",
      "        Data Science       0.88      0.91      0.89        46\n",
      "              DevOps       1.00      0.83      0.91         6\n",
      "          Enrollment       0.33      0.40      0.36         5\n",
      "       Miscellaneous       0.82      0.56      0.67        16\n",
      "  Mobile Development       0.89      1.00      0.94         8\n",
      "Software Engineering       0.44      0.40      0.42        10\n",
      "               UI/UX       0.67      1.00      0.80         4\n",
      "\n",
      "            accuracy                           0.79       105\n",
      "           macro avg       0.74      0.79      0.74       105\n",
      "        weighted avg       0.81      0.79      0.79       105\n",
      "\n",
      "\n",
      "XGBoost Model Confusion Matrix:\n",
      " [[ 5  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  1  0  0  0]\n",
      " [ 0  1  0 42  0  0  1  0  2  0]\n",
      " [ 0  0  0  0  5  0  0  0  0  1]\n",
      " [ 0  0  0  2  0  2  0  0  1  0]\n",
      " [ 0  0  0  1  0  3  9  1  2  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0]\n",
      " [ 0  1  0  3  0  1  0  0  4  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  4]]\n",
      "\n",
      "Accuracy for XGBoost Model: 0.7904761904761904\n"
     ]
    }
   ],
   "source": [
    "# Create an XGBoost model\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predictions on the test set using the XGBoost model\n",
    "xgb_predictions = xgb_model.predict(X_test_vectorized)\n",
    "\n",
    "# Classification report for the XGBoost model\n",
    "xgb_classification_report = classification_report(y_test, xgb_predictions)\n",
    "\n",
    "# Confusion matrix for the XGBoost model\n",
    "xgb_confusion_matrix = confusion_matrix(y_test, xgb_predictions)\n",
    "\n",
    "# Accuracy for the XGBoost model\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "\n",
    "print(\"XGBoost Model Classification Report:\\n\", xgb_classification_report)\n",
    "print(\"\\nXGBoost Model Confusion Matrix:\\n\", xgb_confusion_matrix)\n",
    "print(\"\\nAccuracy for XGBoost Model:\", xgb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for XGBoost Model: 0.8285714285714286\n",
    "\n",
    "The XGBoost model achieved an accuracy of 82.86%. Here are some key performance metrics for specific classes:\n",
    "\n",
    "**High Performance (Precision, Recall, F1-Score):**\n",
    "- 'Event': 100% precision, recall, and F1-score.\n",
    "- 'Mpesa_Mini_Apps_API_Course_Information', 'aws_cloud_course', 'aws_cloud_practitioner', 'cloud_computing', 'cloud_developer_role', 'Software_Engineering_Mobile_Track', 'ai_integration': All achieved perfect scores.\n",
    "\n",
    "**Low Performance (Precision, Recall, F1-Score):**\n",
    "- Several classes, such as 'Data_Science_Career', 'Data_Science_Courses', 'Data_Science_Industry_Relevance', 'Data_Science_Networking_and_Collaboration_Opportunities', and 'learning_resources', showed low scores, indicating room for improvement.\n",
    "\n",
    "**Overall Observation:**\n",
    "The model struggles with certain classes, potentially requiring further tuning or additional data for those categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing XGBoost Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tag: Miscellaneous\n"
     ]
    }
   ],
   "source": [
    "# Test question\n",
    "test_question = \"How many students are in Moringa school\"\n",
    "\n",
    "# Tokenize and preprocess the test question\n",
    "test_tokens = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(test_question) if word.isalnum() and word not in string.punctuation and word not in stop_words]\n",
    "\n",
    "# Convert tokens back to a string\n",
    "cleaned_test_question = ' '.join(test_tokens)\n",
    "\n",
    "# Vectorize the cleaned test question using the same generic vectorizer\n",
    "test_question_vectorized = vectorizer.transform([cleaned_test_question])\n",
    "\n",
    "# Make a prediction using the trained XGBoost model\n",
    "predicted_tag = xgb_model.predict(test_question_vectorized)[0]\n",
    "\n",
    "print(\"Predicted Tag:\", predicted_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "1. **Multinomial Naive Bayes Model:**\n",
    "   - Achieved an accuracy of 66.67%.\n",
    "   - Classes showed varying precision and recall values, highlighting potential areas for improvement.\n",
    "   - Precision measures positive prediction accuracy, while recall indicates coverage of actual positive instances.\n",
    "   - Adjustments may benefit classes with low precision or recall.\n",
    "\n",
    "2. **GridSearchCV Multinomial Naive Bayes:**\n",
    "   - Achieved an overall accuracy of 73.33%.\n",
    "   - Notable variations in precision and recall across different classes.\n",
    "   - Specific classes exhibited high precision and low recall, emphasizing the need for adjustments.\n",
    "   - The confusion matrix aids in identifying correct and incorrect predictions for each class.\n",
    "\n",
    "3. **TF-IDF Vectorizer and SVM Classifier:**\n",
    "   - Demonstrated an outstanding accuracy of 81.9%.\n",
    "   - Varied precision and recall values across classes.\n",
    "   - Confusion matrix insights help understand correct and incorrect predictions.\n",
    "   - Consideration of hyperparameter tuning or additional data may enhance performance.\n",
    "\n",
    "4. **Cross-validation with TF-IDF + SVM:**\n",
    "   - Achieved a solid average cross-validation accuracy of approximately 80.2%.\n",
    "   - High precision and moderate recall in some classes.\n",
    "   - Challenges observed in precision and recall for certain classes.\n",
    "   - Confusion matrix insights provide details on true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "5. **GridSearchCV for SVM:**\n",
    "   - Best SVM model achieved an accuracy of 81.9%.\n",
    "   - Varied precision and recall across classes.\n",
    "   - Classes like Cloud Computing, Contacts, and UI/UX showed high precision and recall.\n",
    "   - Consideration of hyperparameter tuning may further enhance performance.\n",
    "\n",
    "6. **Random Forest Classifier:**\n",
    "   - Achieved a commendable accuracy of around 81%.\n",
    "   - High precision and recall in certain classes.\n",
    "   - Challenges observed in precision and recall for Cybersecurity, Enrollment, and Software Engineering.\n",
    "   - Consideration of hyperparameter tuning or addressing class imbalances may improve performance.\n",
    "\n",
    "7. **GridSearchCV for Random Forest Classifier:**\n",
    "   - Best Random Forest model achieved an average cross-validation accuracy of 81.87%.\n",
    "   - Class-specific insights provided, indicating areas for improvement.\n",
    "   - Confusion matrix breakdown aids in identifying correct and incorrect predictions.\n",
    "\n",
    "8. **XGBoost Model:**\n",
    "   - Achieved an accuracy of 83.86%.\n",
    "   - Varied performance across classes.\n",
    "   - Some classes showed high precision, recall, and F1-score, while others indicated room for improvement.\n",
    "   - Further tuning or additional data may be beneficial for certain categories.\n",
    "\n",
    "Overall, the evaluation of different models and techniques provides insights into their strengths and areas for refinement. Consideration of hyperparameter tuning, adjustments to class-specific challenges, and iterative model refinement are suggested for enhancing overall performance in text classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
